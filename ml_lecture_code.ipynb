{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b2302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all the required libraries\n",
    "!pip install numpy pandas scikit-learn statsmodels econml lightgbm matplotlib seaborn dowhy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from econml.dml import LinearDML, CausalForestDML\n",
    "from econml.metalearners import TLearner, SLearner, XLearner\n",
    "from econml.inference import BootstrapInference\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import dowhy\n",
    "import dowhy.datasets\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Bootstrap replicates for inference (small for speed, increase for better CIs)\n",
    "breps = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56f200",
   "metadata": {},
   "source": [
    "## Causal ML:\n",
    "\n",
    "Causal ML particularly useful when:\n",
    "\n",
    "1. Large number of features\n",
    "2. Complex functional form of Confounders.\n",
    "\n",
    "Let's set up the following simulated data to showcase the advantages of Causal ML, more specifically Double-Debiased Machine Learning.\n",
    "\n",
    "## Data Generating Process (DGP)\n",
    "\n",
    "The simulation follows these key steps:\n",
    "\n",
    "### 1. Covariates (Features)\n",
    "First, we generate a matrix of 15 covariates, $\\mathbf{X}$, for each of the $N$ individuals. Each covariate is drawn independently from a standard normal distribution:\n",
    "\n",
    "$$X_{ij} \\sim \\mathcal{N}(0, 1) \\quad \\text{for } j = 0, 1, \\dots, 99$$\n",
    "\n",
    "***\n",
    "\n",
    "### 2. Confounding and Treatment Assignment\n",
    "A complex, non-linear **confounding term**, $C_i$, is created using the first four covariates. This term will influence both the treatment assignment and the outcome, which is the definition of a confounder.\n",
    "\n",
    "The confounding term $C_i$ is defined as:\n",
    "$$C_i = \\sin(2X_{i0}) + \\cos(2X_{i1}) + X_{i2}^3 + 0.5 X_{i3}^2$$\n",
    "The **treatment assignment**, $D_i$, is then determined by a Bernoulli trial where the probability of receiving the treatment (the propensity score, $f(\\mathbf{X}_i)$) is a logistic function of this confounding term.\n",
    "\n",
    "$$P(D_i=1 | \\mathbf{X}_i) = f(\\mathbf{X}_i) = \\frac{1}{1 + \\exp(-1.5(C_i - 0.5))}$$\n",
    "So, $D_i \\sim \\text{Bernoulli}(e(\\mathbf{X}_i))$.\n",
    "\n",
    "***\n",
    "\n",
    "### 3. Outcome Model\n",
    "Finally, the **outcome model** for $Y_i$ is a linear combination of the treatment effect and a function of the covariates, $g(\\mathbf{X}_i)$, plus a random error term.\n",
    "\n",
    "The complete outcome model is:$$Y_i = \\alpha + \\tau D_i + g(\\mathbf{X}_i) + \\epsilon_i$$\n",
    "Where the true ATE is $\\tau = 10$ and the error term is drawn from a standard normal distribution, $\\epsilon_i \\sim \\mathcal{N}(0, 1)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31546f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Set up a More Robust Simulation ---\n",
    "\n",
    "true_ate = 10.0\n",
    "n_samples = 100000\n",
    "n_features = 15\n",
    "np.random.seed(123)\n",
    "\n",
    "X_data = np.random.normal(0, 1, size=(n_samples, n_features))\n",
    "X = pd.DataFrame(\n",
    "    X_data,\n",
    "    columns=[f'X{i}' for i in range(n_features)]\n",
    ")\n",
    "\n",
    "# --- 2. Define a DGP with STRONGER Confounding ---\n",
    "\n",
    "# This non-linear term will now have a much larger effect\n",
    "confounders = np.sin(X['X0'] * 2) + np.cos(X['X1'] * 2) + (X['X2']**3) + 0.5 * X['X3']**2\n",
    "\n",
    "# Treatment assignment with confounding\n",
    "propensity_score = 1 / (1 + np.exp(-1.5 * (confounders - 0.5)))\n",
    "D = np.random.binomial(1, p=propensity_score)\n",
    "\n",
    "# Outcome model with strong confounding\n",
    "# Both treatment assignment AND outcome depend on the same confounders\n",
    "g_X = 3 * confounders + 0.5 * X['X4']  # Outcome function depends on confounders\n",
    "Y = true_ate * D + g_X + np.random.normal(0, 1, n_samples)\n",
    "\n",
    "print(\"--- Data Simulation Setup ---\")\n",
    "print(f\"True Average Treatment Effect (ATE): {true_ate}\")\n",
    "print(f\"Sample size: {n_samples:,}\")\n",
    "print(f\"Treatment prevalence: {D.mean():.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1b2ef",
   "metadata": {},
   "source": [
    "## OLS Estimation:\n",
    "\n",
    "Let's first estimate the model with OLS. . The standard OLS approach models the outcome $Y$ as a linear function of the treatment $D$ and the covariates $\\mathbf{X}$: $$ Y_i = \\alpha + \\tau D_i + \\boldsymbol{\\beta}'\\mathbf{X}_i + u_i $$\n",
    "\n",
    "The coefficient $\\tau$ represents the ATE estimate. The highly non-linear DGP mechanically creates a **OVB**, yielding a biased ATE estimate. The effect of **OVB** is ambiguous, i.e. upward or downward bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f536aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Model 1: Naive OLS ---\n",
    "\n",
    "# RHS of estimation equation\n",
    "X_with_const = sm.add_constant(np.c_[D, X])\n",
    "\n",
    "# Instantiate OLS model class and fit the model\n",
    "ols_model = sm.OLS(Y, X_with_const).fit()\n",
    "\n",
    "# Extract ATE parameter\n",
    "ols_ate_estimate = ols_model.params[1]\n",
    "\n",
    "# The treatment effect is the coefficient for D, which should be index 1 (after constant)\n",
    "treatment_param_idx = 1\n",
    "ols_conf_interval = ols_model.conf_int().iloc[treatment_param_idx]\n",
    "\n",
    "print(\"--- 1. Naive OLS Results ---\")\n",
    "print(f\"Estimated ATE with OLS: {ols_ate_estimate:.4f}\")\n",
    "print(f\"OLS 95% Confidence Interval: [{ols_conf_interval[0]:.4f}, {ols_conf_interval[1]:.4f}]\")\n",
    "print(f\"Bias: {ols_ate_estimate - true_ate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473f06e1",
   "metadata": {},
   "source": [
    "### OLS Results:\n",
    "\n",
    "Upward bias in ATE estimates ($\\hat{\\tau} = 13.808>10$), where the bias is $\\hat{\\tau} - \\tau_{\\text{true}} = 3.8$. Now let's look at the result from DML.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9b4ca8",
   "metadata": {},
   "source": [
    "# Meta-Learners, Causal Forest and Double/Debiased ML\n",
    "\n",
    "1. **S-Learner (Single Learner)**: Uses a single model with treatment as a feature\n",
    "2. **T-Learner (Two Learner)**: Trains separate models for treated and control groups  \n",
    "3. **X-Learner**: Advanced method that combines T-Learner with cross-fitting\n",
    "4. **Causal Forest**: Tree-based method for heterogeneous treatment effects\n",
    "\n",
    "Each method has different assumptions and performance characteristics depending on the data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca76da",
   "metadata": {},
   "source": [
    "## S-Learner (Single Learner)\n",
    "\n",
    "The S-Learner approach uses a single machine learning model to predict the outcome $Y$ using both the treatment indicator $D$ and covariates $\\mathbf{X}$ as features:\n",
    "\n",
    "$$\\hat{\\mu}(d, \\mathbf{x}) = \\mathbb{E}[Y | D = d, \\mathbf{X} = \\mathbf{x}]$$\n",
    "\n",
    "The ATE is then estimated by predicting counterfactual outcomes:\n",
    "$$\\hat{\\tau}_{\\text{S}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\hat{\\mu}(1, \\mathbf{X}_i) - \\hat{\\mu}(0, \\mathbf{X}_i) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- EconML S-Learner with Bootstrap Inference ---\")\n",
    "\n",
    "# Create S-learner with bootstrap inference for confidence intervals\n",
    "# Bootstrap inference provides theoretically sound confidence intervals\n",
    "s_learner_econml = SLearner(\n",
    "    overall_model=LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        num_leaves=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=123,\n",
    "        verbosity=-1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Fit with bootstrap inference enabled\n",
    "# n_bootstrap_samples controls the number of bootstrap samples (higher = more accurate but slower)\n",
    "s_learner_econml.fit(\n",
    "    Y=Y, \n",
    "    T=D, \n",
    "    X=X, \n",
    "    inference=BootstrapInference(n_bootstrap_samples=breps)\n",
    ")\n",
    "\n",
    "# Calculate ATE point estimate\n",
    "slearner_ate = s_learner_econml.ate(X=X)\n",
    "\n",
    "# Get confidence interval for ATE using built-in method\n",
    "# This uses proper bootstrap inference to compute confidence intervals\n",
    "slearner_ci_lower, slearner_ci_upper = s_learner_econml.ate_interval(X=X, alpha=0.05)\n",
    "\n",
    "# Extract individual treatment effects (CATEs) for each observation\n",
    "slearner_cate = s_learner_econml.effect(X)\n",
    "\n",
    "print(f\"S-learner ATE: {slearner_ate:.4f}\")\n",
    "print(f\"95% Confidence Interval: [{slearner_ci_lower:.4f}, {slearner_ci_upper:.4f}]\")\n",
    "print(f\"Bias: {slearner_ate - true_ate:.4f}\")\n",
    "print(f\"CATE std: {np.std(slearner_cate):.4f}\")\n",
    "print(f\"CATE range: [{np.min(slearner_cate):.4f}, {np.max(slearner_cate):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b619f",
   "metadata": {},
   "source": [
    "## T-Learner (Two Learner)\n",
    "\n",
    "The T-Learner approach trains two separate models: one for the treated group and one for the control group:\n",
    "\n",
    "- **Control model**: $\\hat{\\mu}_0(x) = \\mathbb{E}[Y | D = 0, \\mathbf{X} = x]$\n",
    "- **Treatment model**: $\\hat{\\mu}_1(x) = \\mathbb{E}[Y | D = 1, \\mathbf{X} = x]$\n",
    "\n",
    "The ATE is estimated as:\n",
    "$$\\hat{\\tau}(x) = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ \\hat{\\mu}_1(\\mathbf{X}_i) - \\hat{\\mu}_0(\\mathbf{X}_i) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ecf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EconML T-Learner with Bootstrap Inference ---\n",
    "\n",
    "print(\"--- EconML T-Learner with Bootstrap Inference ---\")\n",
    "\n",
    "# Create T-learner with bootstrap inference for confidence intervals\n",
    "# Bootstrap inference provides theoretically sound confidence intervals\n",
    "t_learner_econml = TLearner(\n",
    "    models=LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        num_leaves=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=123,\n",
    "        verbosity=-1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Fit with bootstrap inference enabled\n",
    "# n_bootstrap_samples controls the number of bootstrap samples (higher = more accurate but slower)\n",
    "t_learner_econml.fit(\n",
    "    Y=Y, \n",
    "    T=D, \n",
    "    X=X, \n",
    "    inference=BootstrapInference(n_bootstrap_samples=breps)\n",
    ")\n",
    "\n",
    "# Calculate ATE point estimate\n",
    "t_learner_ate = t_learner_econml.ate(X=X)\n",
    "\n",
    "# Get confidence interval for ATE using built-in method\n",
    "# This uses proper bootstrap inference to compute confidence intervals\n",
    "t_learner_ci_lower, t_learner_ci_upper = t_learner_econml.ate_interval(X=X, alpha=0.05)\n",
    "\n",
    "# Extract individual treatment effects (CATEs) for each observation\n",
    "t_learner_cate = t_learner_econml.effect(X)\n",
    "\n",
    "print(f\"T-learner ATE: {t_learner_ate:.4f}\")\n",
    "print(f\"95% Confidence Interval: [{t_learner_ci_lower:.4f}, {t_learner_ci_upper:.4f}]\")\n",
    "print(f\"Bias: {t_learner_ate - true_ate:.4f}\")\n",
    "print(f\"CATE std: {np.std(t_learner_cate):.4f}\")\n",
    "print(f\"CATE range: [{np.min(t_learner_cate):.4f}, {np.max(t_learner_cate):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ec3bf",
   "metadata": {},
   "source": [
    "## X-Learner\n",
    "\n",
    "The X-Learner is a more sophisticated meta-learner that combines the T-Learner approach with additional modeling steps to reduce bias. It's particularly effective when treatment and control groups have different sizes.\n",
    "\n",
    "#### The X-Learner procedure:\n",
    "1. Estimate $\\hat{\\mu}_0$ and $\\hat{\\mu}_1$ like in T-Learner\n",
    "2. Compute **pseudo-outcomes** for treatment: $\\tilde{D}_i^1 = Y_i - \\hat{\\mu}_0(X_i)$ for treated units \n",
    "    - Subtract imputed counterfactuals from actual outcomes \n",
    "3. Compute **pseudo-outcomes** for control: $\\tilde{D}_i^0 = \\hat{\\mu}_1(X_i) - Y_i$ for control units \n",
    "    - Subtract imputed counterfactual from the treated counterfactual from actual outcomes\n",
    "4. Train 2 models for the treatment areas:\n",
    "    - **CATE MODEL 1**: $\\hat{\\tau}_1(x) \\leftarrow \\text{Fit model on} \\left\\{(X_i, \\tilde{D}^1_i)|D_i = 1 \\right\\}$\n",
    "    - **CATE MODEL 2**: $\\hat{\\tau}_0(x) \\leftarrow \\text{Fit model on} \\left\\{(X_i, \\tilde{D}^0_i)|D_i = 0 \\right\\}$\n",
    "5. Combine estimates using **estimated** propensity scores:\n",
    "    - **Final CATE Estimate**: $$\n",
    "\\hat{\\tau}(x) = \\overbrace{\\hat{f}(x)}^{\\substack{\\text{Probability of} \\\\ \\text{being treated}}} \\cdot \\overbrace{\\hat{\\tau}_0(x)}^{\\substack{\\text{CATE model derived from} \\\\ \\text{the NOT treated group}}} + \\overbrace{(1-\\hat{f}(x))}^{\\substack{\\text{Probability of} \\\\ \\text{NOT being treated}}} \\cdot \\overbrace{\\hat{\\tau}_1(x)}^{\\substack{\\text{CATE model derived from} \\\\ \\text{the TREATED group}}}$$\n",
    "\n",
    "**Main Goal**: Weighted average of CATEs where more trust is on the more populated group (great for treatment areas imbalance) $\\rightarrow$ overweight more populated group for more confident treatment estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EconML X-Learner with Bootstrap Inference ---\n",
    "\n",
    "print(\"--- EconML X-Learner with Bootstrap Inference ---\")\n",
    "\n",
    "# Create X-learner with bootstrap inference for confidence intervals\n",
    "# Bootstrap inference provides theoretically sound confidence intervals\n",
    "x_learner_econml = XLearner(\n",
    "    # Model for Pseudo-outcomes\n",
    "    models=LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        num_leaves=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=123,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    # Model for Propensity Scores\n",
    "    propensity_model=LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        num_leaves=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=123,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    # Model CATEs (regress pseudo-outcomes on X in each treatment group)\n",
    "    cate_models=LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        num_leaves=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=123,\n",
    "        verbosity=-1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Fit with bootstrap inference enabled\n",
    "# n_bootstrap_samples controls the number of bootstrap samples (higher = more accurate but slower)\n",
    "x_learner_econml.fit(\n",
    "    Y=Y, \n",
    "    T=D, \n",
    "    X=X, \n",
    "    inference=BootstrapInference(n_bootstrap_samples=10)\n",
    ")\n",
    "\n",
    "# Calculate ATE point estimate\n",
    "x_learner_ate = x_learner_econml.ate(X=X)\n",
    "\n",
    "# Get confidence interval for ATE using built-in method\n",
    "# This uses proper bootstrap inference to compute confidence intervals\n",
    "x_learner_ci_lower, x_learner_ci_upper = x_learner_econml.ate_interval(X=X, alpha=0.05)\n",
    "\n",
    "# Extract individual treatment effects (CATEs) for each observation\n",
    "x_learner_cate = x_learner_econml.effect(X)\n",
    "\n",
    "print(f\"X-learner ATE: {x_learner_ate:.4f}\")\n",
    "print(f\"95% Confidence Interval: [{x_learner_ci_lower:.4f}, {x_learner_ci_upper:.4f}]\")\n",
    "print(f\"Bias: {x_learner_ate - true_ate:.4f}\")\n",
    "print(f\"CATE std: {np.std(x_learner_cate):.4f}\")\n",
    "print(f\"CATE range: [{np.min(x_learner_cate):.4f}, {np.max(x_learner_cate):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275464da",
   "metadata": {},
   "source": [
    "### Causal Forest\n",
    "\n",
    "Causal Forest is a tree-based method that can capture heterogeneous treatment effects and complex non-linear relationships. It builds on random forests but is specifically designed for causal inference.\n",
    "\n",
    "Key features:\n",
    "- Handles heterogeneous treatment effects naturally\n",
    "- Non-parametric and flexible\n",
    "- Provides honest estimates through sample splitting\n",
    "- Can identify subgroups with different treatment effects\n",
    "\n",
    "***\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "1. Model to predict outcomes $\\rightarrow$ regress Y on X and store residuals $\\tilde{Y}$\n",
    "2. Model to predict treatment $\\rightarrow$ regress D on X and store residuals $\\tilde{D}$\n",
    "3. Model for treatment effect heterogeneity $\\rightarrow$ Causal Trees on the following model $$\\tilde{Y} = \\tau(X) \\tilde{D}$$\n",
    "\n",
    "### Causal Trees (Athey & Imbens, 2016)\n",
    "\n",
    "Main goal is to predict a treatment effect $\\tau$. \n",
    "\n",
    "#### Splitting Logic:\n",
    "Splits the data to make the treatment effect as different as possible between new groups. \n",
    "\n",
    "#### Output: \n",
    "Predicted treatment effect (**CATE**): each final ''leaf'' contains a final treatment effect. The **CATE** is defined as: $$\\tau(x) = \\mathbb{E}\\left[Y^{(1)} - Y^{(0)} |X_i = x_i\\right]$$\n",
    "\n",
    "## The algorithm:\n",
    "\n",
    "1. **Split the data**: Randomly divide the full dataset into a Splitting Sample and an Estimation Sample.\n",
    "2. Use splitting sample to find best split:\n",
    "    - Look for every possible split on every feature, e.g. $\\text{age}\\leq 40$ and $\\text{age}>40$\n",
    "    - Inside each temporary group calculates a rough estimate of the treatment effect: $$\\tau = \\mathbb{E}\\left[Y^{(1)} - Y^{(0)}\\right]$$\n",
    "    - Finally chooses the split maximizing the difference in treatment effects between child nodes\n",
    "\n",
    "3. Repeat this process recursively, splitting each node until a stopping criterion is met\n",
    "4. Having finalized the tree structure estimate the **CATE** using **Estimation Sample**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a22fb3",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Note on Inference:\n",
    "\n",
    "Subforest size is used to create parallel processing to estimate the treatment effects. The number of independent forests will then be n_estimators \\ subforest_size = 100, effetictevly replicating the same analysis 100 times to estimate variance, similar to a bootstrap procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EconML Causal Forest ---\n",
    "\n",
    "print(\"--- EconML Causal Forest---\")\n",
    "\n",
    "# Causal Forest\n",
    "causal_forest_econml = CausalForestDML(\n",
    "    model_y=LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        num_leaves=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=123,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    model_t=LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        num_leaves=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=123,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    \n",
    "    n_estimators=200, # n_estimators must be divisible by subforest_size\n",
    "    subforest_size=5,  # Subforests to compute variance \n",
    "    max_depth=6,      # Reduced depth for faster execution\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    cv=3,  # Cross-fitting folds (reduced for faster execution)\n",
    "    discrete_treatment=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "causal_forest_econml.fit(\n",
    "    Y=Y, \n",
    "    T=D, \n",
    "    X=X, \n",
    ")\n",
    "\n",
    "# Calculate ATE point estimate\n",
    "forest_ate = causal_forest_econml.ate(X=X)\n",
    "\n",
    "# Get confidence interval for ATE using built-in method\n",
    "# This uses proper bootstrap inference to compute confidence intervals\n",
    "forest_ci_lower, forest_ci_upper = causal_forest_econml.ate_interval(X=X, alpha=0.05)\n",
    "\n",
    "# Estimate individual treatment effects for heterogeneity analysis\n",
    "cate_forest = causal_forest_econml.effect(X)\n",
    "\n",
    "print(f\"Causal Forest ATE: {forest_ate:.4f}\")\n",
    "print(f\"95% Confidence Interval: [{forest_ci_lower:.4f}, {forest_ci_upper:.4f}]\")\n",
    "print(f\"Bias: {forest_ate - true_ate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26b138a",
   "metadata": {},
   "source": [
    "## Double/Debiased Machine Learning: Estimation\n",
    "\n",
    "1. Estimate first one flexible model for the outcome only based on confounders ($X_i$) and stores the residuals: $$ \\tilde{Y} = Y - Y_{\\text{predicted}}\\text{.}$$\n",
    "\n",
    "2. Trains a separate model to predict the treatment $D$ on the same confounders and calculates the residuals: $$\\tilde{D} = D - D_{\\text{prediction}}\\text{.}$$ These residuals represent the part of the treatment that is not explained by the confounders.\n",
    "\n",
    "By doing so we remove the effect of confounders on both the **outcome** and **treatment assignment**\n",
    "\n",
    "**CAVEAT**: To avoid overfitting use **K-Fold crossfitting** for the predictions, split data in K-fold and use K-1 fold for training and last fold for predictions iteratively.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0401d37",
   "metadata": {},
   "source": [
    "## Final Step:\n",
    "\n",
    "**Debiased Estimation**: Run a simple linear regression on just the residuals $\\tilde{Y}$ and $\\tilde{D}$: $$\\tilde{Y_i} = \\beta_0 + \\tau\\tilde{D_i} + u_i.$$ Now $\\hat{\\tau}$ is our ATE coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0bd266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Model 2: Double Machine Learning ---\n",
    "\n",
    "# Instantiating the K-Fold cross-validator\n",
    "cv_splitter = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# Instantiate the LinearDML model with LightGBM\n",
    "est_dml = LinearDML(\n",
    "    model_y=LGBMRegressor(\n",
    "        n_estimators=100, \n",
    "        num_leaves=30, \n",
    "        learning_rate=0.1,\n",
    "        random_state=123,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    model_t=LGBMClassifier(\n",
    "        n_estimators=100, \n",
    "        num_leaves=30, \n",
    "        learning_rate=0.1,\n",
    "        random_state=123,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    # Cross-fitting\n",
    "    cv=cv_splitter,\n",
    "    discrete_treatment=True,\n",
    "    # Replication seed\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "est_dml.fit(\n",
    "    Y, T=D, X=X\n",
    ")\n",
    "\n",
    "ate_summary = est_dml.ate_inference(X=X)\n",
    "dml_ate_estimate = ate_summary.mean_point\n",
    "dml_ci_lower, dml_ci_upper = ate_summary.conf_int_mean(alpha=0.05)\n",
    "\n",
    "# Extract individual treatment effects (CATEs) for each observation\n",
    "dml_cate = est_dml.effect(X)\n",
    "\n",
    "print(\"--- 2. Double Machine Learning Results ---\")\n",
    "print(f\"Estimated ATE: {dml_ate_estimate:.4f}\")\n",
    "print(f\"P-value: {ate_summary.pvalue():.6f}\")\n",
    "print(f\"95% Confidence Interval: [{dml_ci_lower:.4f}, {dml_ci_upper:.4f}]\")\n",
    "print(f\"Bias: {dml_ate_estimate - true_ate:.4f}\")\n",
    "print(f\"CATE std: {np.std(dml_cate):.4f}\")\n",
    "print(f\"CATE range: [{np.min(dml_cate):.4f}, {np.max(dml_cate):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4fcac4",
   "metadata": {},
   "source": [
    "## Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aee1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Visualize Results ---\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Data for plotting - all methods with updated variable names\n",
    "methods = ['OLS', 'S-Learner', 'T-Learner', 'X-Learner', 'Causal Forest', 'DML']\n",
    "estimates = [\n",
    "    ols_ate_estimate, \n",
    "    slearner_ate, \n",
    "    t_learner_ate, \n",
    "    x_learner_ate, \n",
    "    forest_ate, \n",
    "    dml_ate_estimate\n",
    "]\n",
    "ci_lowers = [\n",
    "    ols_conf_interval[0], \n",
    "    slearner_ci_lower, \n",
    "    t_learner_ci_lower, \n",
    "    x_learner_ci_lower, \n",
    "    forest_ci_lower, \n",
    "    dml_ci_lower\n",
    "]\n",
    "ci_uppers = [\n",
    "    ols_conf_interval[1], \n",
    "    slearner_ci_upper, \n",
    "    t_learner_ci_upper, \n",
    "    x_learner_ci_upper, \n",
    "    forest_ci_upper, \n",
    "    dml_ci_upper\n",
    "]\n",
    "\n",
    "# Color scheme: Using seaborn Pastel2 palette for better aesthetics\n",
    "colors = sns.color_palette(\"Set2\", len(methods))\n",
    "\n",
    "# Create horizontal error bars\n",
    "y_positions = list(range(len(methods)))\n",
    "for i, (method, est, ci_low, ci_up, color) in enumerate(zip(methods, estimates, ci_lowers, ci_uppers, colors)):\n",
    "    # Error bar\n",
    "    ax.errorbar(est, y_positions[i], \n",
    "                xerr=[[est - ci_low], [ci_up - est]], \n",
    "                fmt='o', color=color, capsize=8, capthick=2,\n",
    "                markersize=10, linewidth=2, label=f'{method}: {est:.3f}')\n",
    "\n",
    "# Add vertical line for true ATE\n",
    "ax.axvline(x=true_ate, color='black', linestyle='--', linewidth=1, \n",
    "           label=f'True ATE: {true_ate}', alpha=0.8)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_yticks(y_positions)\n",
    "ax.set_yticklabels(methods, fontsize=12)\n",
    "ax.set_xlabel('Treatment Effect Estimate', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Comparison of Causal Inference Methods: Treatment Effect Estimates', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Remove top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add grid for better readability\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='upper right', fontsize=10, frameon=True, fancybox=True, shadow=True)\n",
    "\n",
    "# Set x-axis limits to better show the confidence intervals\n",
    "all_ci_values = ci_lowers + ci_uppers\n",
    "x_min = min(all_ci_values) - 1\n",
    "x_max = max(all_ci_values) + 1\n",
    "ax.set_xlim(x_min, x_max)\n",
    "\n",
    "# Add some padding to y-axis\n",
    "ax.set_ylim(-0.5, len(methods) - 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: Causal Inference Methods Comparison\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Method':<15} {'ATE Estimate':<12} {'Bias':<8} {'95% CI Lower':<12} {'95% CI Upper':<12}\")\n",
    "print(\"-\"*80)\n",
    "for method, est, ci_low, ci_up in zip(methods, estimates, ci_lowers, ci_uppers):\n",
    "    bias = est - true_ate\n",
    "    print(f\"{method:<15} {est:<12.4f} {bias:<8.4f} {ci_low:<12.4f} {ci_up:<12.4f}\")\n",
    "print(f\"{'True ATE':<15} {true_ate:<12.4f} {'0.0000':<8} {'':<12} {'':<12}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c955438",
   "metadata": {},
   "source": [
    "***\n",
    "# CATEs: \n",
    "## Focus on Conditional Average Treatment Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f747404",
   "metadata": {},
   "source": [
    "# Real-World Application: The Lalonde NSW Dataset\n",
    "\n",
    "Now let's apply our causal ML methods to **real data** to see how they perform in practice. We'll use the famous **National Supported Work (NSW) dataset** analyzed by Robert Lalonde.\n",
    "\n",
    "## The Dataset: Job Training Program Evaluation\n",
    "\n",
    "The NSW program was a job training initiative from the 1970s designed to help disadvantaged workers. The key research question is:\n",
    "\n",
    "**Did participation in the job training program causally increase participants' earnings?**\n",
    "\n",
    "### Key Variables:\n",
    "- **Treatment (`treat`)**: Binary indicator (1 = participated in program, 0 = control group)\n",
    "- **Outcome (`re78`)**: Real earnings in 1978 (post-program)\n",
    "- **Covariates**: Pre-treatment characteristics including:\n",
    "  - Demographics: `age`, `educ` (education), `black`, `hisp`, `married`\n",
    "  - Employment history: `re74`, `re75` (earnings in 1974-1975)\n",
    "  - Education status: `nodegree` (no high school degree)\n",
    "\n",
    "This is a **classic causal inference dataset** where we expect **genuine heterogeneous treatment effects** - the program likely helps some individuals more than others based on their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e1929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Lalonde dataset\n",
    "df_lalonde = dowhy.datasets.lalonde_dataset()\n",
    "\n",
    "print(\"=== LALONDE NSW DATASET OVERVIEW ===\")\n",
    "print(f\"Dataset shape: {df_lalonde.shape}\")\n",
    "print(f\"\\nDataset information:\")\n",
    "df_lalonde.info()\n",
    "\n",
    "# Define variables for the Lalonde dataset\n",
    "outcome_lalonde = 're78'      # Real earnings in 1978 (outcome)\n",
    "treatment_lalonde = 'treat'   # 1 if in the program, 0 otherwise (treatment)\n",
    "covariates_lalonde = [col for col in df_lalonde.columns if col not in [outcome_lalonde, treatment_lalonde]]\n",
    "\n",
    "# Separate variables for clarity\n",
    "Y_lalonde = df_lalonde[outcome_lalonde]\n",
    "T_lalonde = df_lalonde[treatment_lalonde]\n",
    "X_lalonde = df_lalonde[covariates_lalonde]\n",
    "\n",
    "print(f\"\\nTreatment distribution:\")\n",
    "print(f\"Number of treated units: {T_lalonde.sum()}\")\n",
    "print(f\"Number of control units: {len(df_lalonde) - T_lalonde.sum()}\")\n",
    "print(f\"Treatment rate: {T_lalonde.mean():.1%}\")\n",
    "\n",
    "print(f\"\\nOutcome statistics:\")\n",
    "print(f\"Mean earnings (treated): ${Y_lalonde[T_lalonde==1].mean():.2f}\")\n",
    "print(f\"Mean earnings (control): ${Y_lalonde[T_lalonde==0].mean():.2f}\")\n",
    "print(f\"Naive difference: ${Y_lalonde[T_lalonde==1].mean() - Y_lalonde[T_lalonde==0].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nCovariates: {covariates_lalonde}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\nFirst 5 rows of the dataset:\")\n",
    "df_lalonde.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c9a40",
   "metadata": {},
   "source": [
    "## Applying Causal ML Methods to Real Data\n",
    "\n",
    "Now let's apply all our causal ML methods to the Lalonde dataset to estimate both the ATE and individual treatment effects (CATEs). Real-world data often exhibits **genuine heterogeneity** - some individuals benefit more from treatment than others, making CATE analysis particularly valuable for policy targeting.\n",
    "\n",
    "### S-Learner on Lalonde Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a5f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== S-LEARNER ON LALONDE DATA ===\")\n",
    "\n",
    "# Create S-learner for Lalonde dataset\n",
    "s_learner_lalonde = SLearner(\n",
    "    overall_model=LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        num_leaves=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "s_learner_lalonde.fit(\n",
    "    Y=Y_lalonde, \n",
    "    T=T_lalonde, \n",
    "    X=X_lalonde, \n",
    "    inference=BootstrapInference(n_bootstrap_samples=breps)\n",
    ")\n",
    "\n",
    "# Get ATE and confidence intervals\n",
    "slearner_ate_lalonde = s_learner_lalonde.ate(X=X_lalonde)\n",
    "slearner_ci_lower_lalonde, slearner_ci_upper_lalonde = s_learner_lalonde.ate_interval(X=X_lalonde, alpha=0.05)\n",
    "\n",
    "# Get individual treatment effects (CATEs)\n",
    "slearner_cate_lalonde = s_learner_lalonde.effect(X_lalonde)\n",
    "\n",
    "print(f\"S-learner ATE: ${slearner_ate_lalonde:.2f}\")\n",
    "print(f\"95% Confidence Interval: [${slearner_ci_lower_lalonde:.2f}, ${slearner_ci_upper_lalonde:.2f}]\")\n",
    "print(f\"CATE std: ${np.std(slearner_cate_lalonde):.2f}\")\n",
    "print(f\"CATE range: [${np.min(slearner_cate_lalonde):.2f}, ${np.max(slearner_cate_lalonde):.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f57f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== T-LEARNER ON LALONDE DATA ===\")\n",
    "\n",
    "# Create T-learner for Lalonde dataset\n",
    "t_learner_lalonde = TLearner(\n",
    "    models=LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        num_leaves=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "t_learner_lalonde.fit(\n",
    "    Y=Y_lalonde, \n",
    "    T=T_lalonde, \n",
    "    X=X_lalonde, \n",
    "    inference=BootstrapInference(n_bootstrap_samples=breps)\n",
    ")\n",
    "\n",
    "# Get ATE and confidence intervals\n",
    "t_learner_ate_lalonde = t_learner_lalonde.ate(X=X_lalonde)\n",
    "t_learner_ci_lower_lalonde, t_learner_ci_upper_lalonde = t_learner_lalonde.ate_interval(X=X_lalonde, alpha=0.05)\n",
    "\n",
    "# Get individual treatment effects (CATEs)\n",
    "t_learner_cate_lalonde = t_learner_lalonde.effect(X_lalonde)\n",
    "\n",
    "print(f\"T-learner ATE: ${t_learner_ate_lalonde:.2f}\")\n",
    "print(f\"95% Confidence Interval: [${t_learner_ci_lower_lalonde:.2f}, ${t_learner_ci_upper_lalonde:.2f}]\")\n",
    "print(f\"CATE std: ${np.std(t_learner_cate_lalonde):.2f}\")\n",
    "print(f\"CATE range: [${np.min(t_learner_cate_lalonde):.2f}, ${np.max(t_learner_cate_lalonde):.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e76466",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== X-LEARNER ON LALONDE DATA ===\")\n",
    "\n",
    "# Create X-learner for Lalonde dataset\n",
    "x_learner_lalonde = XLearner(\n",
    "    models=LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        num_leaves=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    propensity_model=LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        num_leaves=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    cate_models=LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        num_leaves=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "x_learner_lalonde.fit(\n",
    "    Y=Y_lalonde, \n",
    "    T=T_lalonde, \n",
    "    X=X_lalonde, \n",
    "    inference=BootstrapInference(n_bootstrap_samples=breps)\n",
    ")\n",
    "\n",
    "# Get ATE and confidence intervals\n",
    "x_learner_ate_lalonde = x_learner_lalonde.ate(X=X_lalonde)\n",
    "x_learner_ci_lower_lalonde, x_learner_ci_upper_lalonde = x_learner_lalonde.ate_interval(X=X_lalonde, alpha=0.05)\n",
    "\n",
    "# Get individual treatment effects (CATEs)\n",
    "x_learner_cate_lalonde = x_learner_lalonde.effect(X_lalonde)\n",
    "\n",
    "print(f\"X-learner ATE: ${x_learner_ate_lalonde:.2f}\")\n",
    "print(f\"95% Confidence Interval: [${x_learner_ci_lower_lalonde:.2f}, ${x_learner_ci_upper_lalonde:.2f}]\")\n",
    "print(f\"CATE std: ${np.std(x_learner_cate_lalonde):.2f}\")\n",
    "print(f\"CATE range: [${np.min(x_learner_cate_lalonde):.2f}, ${np.max(x_learner_cate_lalonde):.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb923f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CAUSAL FOREST ON LALONDE DATA ===\")\n",
    "\n",
    "# Create Causal Forest for Lalonde dataset\n",
    "causal_forest_lalonde = CausalForestDML(\n",
    "    model_y=LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        num_leaves=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    model_t=LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        num_leaves=30,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    n_estimators=500,  # n_estimators must be divisible by subforest_size\n",
    "    subforest_size=5,  # Subforests to compute variance \n",
    "    max_depth=6,      # Reduced depth for faster execution\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    cv=3,  # Cross-fitting folds (reduced for faster execution)\n",
    "    discrete_treatment=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "causal_forest_lalonde.fit(\n",
    "    Y=Y_lalonde, \n",
    "    T=T_lalonde, \n",
    "    X=X_lalonde\n",
    ")\n",
    "\n",
    "# Get ATE and confidence intervals\n",
    "forest_ate_lalonde = causal_forest_lalonde.ate(X=X_lalonde)\n",
    "forest_ci_lower_lalonde, forest_ci_upper_lalonde = causal_forest_lalonde.ate_interval(X=X_lalonde, alpha=0.05)\n",
    "\n",
    "# Get individual treatment effects (CATEs)\n",
    "cate_forest_lalonde = causal_forest_lalonde.effect(X_lalonde)\n",
    "\n",
    "print(f\"Causal Forest ATE: ${forest_ate_lalonde:.2f}\")\n",
    "print(f\"95% Confidence Interval: [${forest_ci_lower_lalonde:.2f}, ${forest_ci_upper_lalonde:.2f}]\")\n",
    "print(f\"CATE std: ${np.std(cate_forest_lalonde):.2f}\")\n",
    "print(f\"CATE range: [${np.min(cate_forest_lalonde):.2f}, ${np.max(cate_forest_lalonde):.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c277061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DOUBLE MACHINE LEARNING ON LALONDE DATA ===\")\n",
    "\n",
    "# Create DML estimator for Lalonde dataset\n",
    "cv_splitter_lalonde = KFold(n_splits=3, shuffle=True, random_state=42)  # Fewer folds for small dataset\n",
    "\n",
    "est_dml_lalonde = LinearDML(\n",
    "    model_y=LGBMRegressor(\n",
    "        n_estimators=100, \n",
    "        num_leaves=30, \n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    model_t=LGBMClassifier(\n",
    "        n_estimators=100, \n",
    "        num_leaves=30, \n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    cv=cv_splitter_lalonde,\n",
    "    discrete_treatment=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "est_dml_lalonde.fit(Y_lalonde, T=T_lalonde, X=X_lalonde)\n",
    "\n",
    "# Get ATE and confidence intervals\n",
    "ate_summary_lalonde = est_dml_lalonde.ate_inference(X=X_lalonde)\n",
    "dml_ate_lalonde = ate_summary_lalonde.mean_point\n",
    "dml_ci_lower_lalonde, dml_ci_upper_lalonde = ate_summary_lalonde.conf_int_mean(alpha=0.05)\n",
    "\n",
    "# Get individual treatment effects (CATEs)\n",
    "dml_cate_lalonde = est_dml_lalonde.effect(X_lalonde)\n",
    "\n",
    "print(f\"DML ATE: ${dml_ate_lalonde:.2f}\")\n",
    "print(f\"P-value: {ate_summary_lalonde.pvalue():.4f}\")\n",
    "print(f\"95% Confidence Interval: [${dml_ci_lower_lalonde:.2f}, ${dml_ci_upper_lalonde:.2f}]\")\n",
    "print(f\"CATE std: ${np.std(dml_cate_lalonde):.2f}\")\n",
    "print(f\"CATE range: [${np.min(dml_cate_lalonde):.2f}, ${np.max(dml_cate_lalonde):.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22761fa9",
   "metadata": {},
   "source": [
    "## CATE Analysis: Heterogeneous Treatment Effects\n",
    "\n",
    "Now let's examine the **heterogeneous treatment effects** in the real Lalonde data. This analysis reveals how different individuals respond to the job training program, providing insights for targeted policy implementation.\n",
    "\n",
    "### Key Focus:\n",
    "- **Individual-level effects**: How much does each person benefit from the job training program?\n",
    "- **Policy targeting**: Which types of individuals should be prioritized for the program?\n",
    "- **Method comparison**: How do different causal ML approaches capture treatment effect heterogeneity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CATE Distribution Analysis for Lalonde Data ---\n",
    "\n",
    "# Prepare data for CATE comparison (Lalonde dataset)\n",
    "cate_data_lalonde = {\n",
    "    'S-Learner': slearner_cate_lalonde.flatten(),\n",
    "    'T-Learner': t_learner_cate_lalonde.flatten(),\n",
    "    'X-Learner': x_learner_cate_lalonde.flatten(),\n",
    "    'Causal Forest': cate_forest_lalonde.flatten(),\n",
    "    'DML': dml_cate_lalonde.flatten()\n",
    "}\n",
    "\n",
    "# Corresponding ATE estimates for vertical lines (Lalonde dataset)\n",
    "ate_estimates_dict_lalonde = {\n",
    "    'S-Learner': slearner_ate_lalonde,\n",
    "    'T-Learner': t_learner_ate_lalonde,\n",
    "    'X-Learner': x_learner_ate_lalonde,\n",
    "    'Causal Forest': forest_ate_lalonde,\n",
    "    'DML': dml_ate_lalonde\n",
    "}\n",
    "\n",
    "# Create comprehensive comparison plot: Real Data (Lalonde)\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Color palette for consistency\n",
    "colors = sns.color_palette(\"Set2\", len(cate_data_lalonde))\n",
    "\n",
    "# Plot KDE for each method on Lalonde data\n",
    "for i, (method, cates) in enumerate(cate_data_lalonde.items()):\n",
    "    # Create KDE plot\n",
    "    sns.kdeplot(data=cates, label=f'{method} (Lalonde)', color=colors[i], \n",
    "                linewidth=2.5, alpha=0.8, ax=ax)\n",
    "    \n",
    "    # Add vertical line for model-specific ATE\n",
    "    ax.axvline(x=ate_estimates_dict_lalonde[method], color=colors[i], linestyle='-', \n",
    "                alpha=0.7, linewidth=1.5)\n",
    "\n",
    "# Calculate overall statistics for Lalonde data\n",
    "all_cates_lalonde = np.concatenate([cates for cates in cate_data_lalonde.values()])\n",
    "median_ate_lalonde = np.median([ate for ate in ate_estimates_dict_lalonde.values()])\n",
    "\n",
    "# Add reference line for median ATE across all methods\n",
    "ax.axvline(x=median_ate_lalonde, color='black', linestyle=':', linewidth=2, \n",
    "           label=f'Median ATE: ${median_ate_lalonde:.0f}', alpha=0.8)\n",
    "\n",
    "# Set axis limits for better visualization\n",
    "ax.set_ylim(0, 0.0005)     # Density scale appropriate for earnings\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel('Conditional Average Treatment Effect (CATE) - Annual Earnings ($)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Density', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Distribution of Individual Treatment Effects (CATEs)\\nLalonde NSW Dataset: Job Training Program', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='upper right', fontsize=11, frameon=True, fancybox=True, shadow=True)\n",
    "\n",
    "# Add grid for better readability\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Set style\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive CATE summary for Lalonde data\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CATE DISTRIBUTION SUMMARY - LALONDE NSW DATASET\")\n",
    "print(\"=\"*100)\n",
    "print(f\"{'Method':<15} {'Mean CATE':<12} {'Median CATE':<12} {'Std CATE':<12} {'Min CATE':<12} {'Max CATE':<12} {'Range':<12}\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for method, cates in cate_data_lalonde.items():\n",
    "    mean_cate = np.mean(cates)\n",
    "    median_cate = np.median(cates)\n",
    "    std_cate = np.std(cates)\n",
    "    min_cate = np.min(cates)\n",
    "    max_cate = np.max(cates)\n",
    "    range_cate = max_cate - min_cate\n",
    "    \n",
    "    print(f\"{method:<15} ${mean_cate:<11.0f} ${median_cate:<11.0f} ${std_cate:<11.0f} ${min_cate:<11.0f} ${max_cate:<11.0f} ${range_cate:<11.0f}\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Additional analysis for Lalonde data\n",
    "print(f\"\\nLALONDE DATASET INSIGHTS:\")\n",
    "print(f\"• Dataset size: {len(Y_lalonde)} individuals\")\n",
    "print(f\"• Treatment rate: {T_lalonde.mean():.1%}\")\n",
    "print(f\"• Average earnings (control): ${Y_lalonde[T_lalonde==0].mean():.0f}\")\n",
    "print(f\"• Average earnings (treated): ${Y_lalonde[T_lalonde==1].mean():.0f}\")\n",
    "print(f\"• Naive difference: ${Y_lalonde[T_lalonde==1].mean() - Y_lalonde[T_lalonde==0].mean():.0f}\")\n",
    "\n",
    "print(f\"\\nHETEROGENEITY ANALYSIS:\")\n",
    "print(\"• Positive CATEs suggest the individual would benefit from the job training program\")\n",
    "print(\"• Negative CATEs suggest the program might actually hurt that individual's earnings\")\n",
    "print(\"• Large standard deviations indicate substantial heterogeneity across individuals\")\n",
    "print(\"• Range shows the difference between most and least benefited individuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac781a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
